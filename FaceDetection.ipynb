{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645bf13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc026ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL - Avoid OOM errors by setting GPU Memory Consumption Growth \n",
    "# If GPU and CUDA has been configured on your PC\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0307b3-b01a-48e9-90ce-8ba932637d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# # Building image loading function\n",
    "def load_image(x):\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img\n",
    "\n",
    "# # Building label loading function\n",
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3662bc-f683-4f37-80bc-8b5d94dd475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Augmented Images to Tensorflow Dataset\n",
    "\n",
    "AUG_DATA = \"aug_data\"\n",
    "TRAIN = \"train\"\n",
    "VAL = \"val\"\n",
    "TEST = \"test\"\n",
    "IMAGES = \"images\"\n",
    "LABELS = \"labels\"\n",
    "\n",
    "resize = 120\n",
    "\n",
    "def get_images(folder):\n",
    "    images = tf.data.Dataset.list_files(os.path.join(AUG_DATA, folder, IMAGES, '*.jpg'), shuffle=False)\n",
    "    images = images.map(load_image)\n",
    "    images = images.map(lambda x: tf.image.resize(x, (resize,resize))) # Resizing to be more efficienty model\n",
    "    images = images.map(lambda x: x/255)  # Apply a range to 255 to apply sigmoid on the next process, to has a equal range\n",
    "    return images\n",
    "\n",
    "train_images = get_images(TRAIN)\n",
    "test_images = get_images(TEST)\n",
    "val_images = get_images(VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684b721-9c35-4dd0-a5ce-6947557e6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL - This is to check train images variable\n",
    "train_images.as_numpy_iterator().next() \n",
    "\n",
    "### OPTIONAL - Only checking if the train_labels cotains correctly files dataset\n",
    "train_labels = tf.data.Dataset.list_files(os.path.join(AUG_DATA, TRAIN, LABELS, '*.json'), shuffle=False)\n",
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44aa519-e800-415d-bcd1-0f181c170688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Labels to TensorFlow Dataset\n",
    "\n",
    "def get_labels(folder):\n",
    "    labels = tf.data.Dataset.list_files(os.path.join(AUG_DATA, folder, LABELS, '*.json'), shuffle=False)\n",
    "    labels = labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))\n",
    "    return labels\n",
    "\n",
    "train_labels = get_labels(TRAIN)\n",
    "test_labels = get_labels(TEST)\n",
    "val_labels = get_labels(VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf60f8c-2ca1-430c-a5f1-ba2919d8735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL - Check variables sizes\n",
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0c7d0-2963-4f41-9a0b-5e460eaf5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine label and Image samples\n",
    "# Create Final Datasets (Images/Labels)\n",
    "\n",
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(1300) # Define always greater than images size (Images size = 11100 Then 12100 is good)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)\n",
    "\n",
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)\n",
    "\n",
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(300)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6030c-ac93-4e27-9476-e89f5c633cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL - Check train shape and content\n",
    "train.as_numpy_iterator().next()[0].shape\n",
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df613372-9450-46a2-8787-d3b2da117633",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL -View Images and Annotations\n",
    "data_samples = train.as_numpy_iterator()\n",
    "res = data_samples.next()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=8, figsize=(20,20))\n",
    "for idx in range(8): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    sample_image = cv2.UMat(sample_image) # Convert to UMat\n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [resize,resize]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [resize,resize]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "    sample_image_display = np.asarray(sample_image.get()) # Convert to np.uint8 to be showed on imshow\n",
    "    ax[idx].imshow(sample_image_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4b490-6fe8-49e8-94e5-65965be31460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build deep learning\n",
    "# Import Layers and Base Network\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade2543-0e1a-4961-bce2-a1f2c27e688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download VGG16\n",
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3cabde-cc08-4d35-9db6-40aae66171da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL - See VGG16 structure\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2db23e-123d-46b0-bff9-305c163dcd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build instance of Network\n",
    "def build_model(): \n",
    "    input_layer = Input(shape=(resize,resize,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model  \n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27896c40-6acc-49cc-b3a2-5a663492a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out Neural Network\n",
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ca9d0-ba7d-4bc3-9de1-c594c349bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL - See model structure\n",
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d69aa-921e-4e63-81fc-f533fbc24f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Losses and Optimizers\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=1000, # This value is not too agressive\n",
    "    decay_rate=0.75,\n",
    "    staircase=True)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bdc75-7b63-4de2-bad0-b0e368dccab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Localization Loss and Classification Loss function\n",
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2])) # Get the difference between y and yhat\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size # Localization loss\n",
    "\n",
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300db3d-9487-49f3-92f5-40550d892785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Network Class\n",
    "class FaceTracker(Model): \n",
    "    def __init__(self, eyetracker,  **kwargs):  # Pre build model\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs): # Compile model (Optmizer, Localization Loss)\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs):  # Train neural network\n",
    "        \n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True) #  Make predictions\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes) # Calculate loss\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)       \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables) # Calculate gradiant\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables)) # Apply gradiant descent \n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919acf1-6678-424b-9723-fb88d5e6649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model = FaceTracker(facetracker)\n",
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06393d-8776-4ab1-9023-62474ff06c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "logdir='logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir) # Allows tracking and visualizing metrics such as loss and accuracy\n",
    "hist = model.fit(train, epochs=30, validation_data=val, callbacks=[tensorboard_callback]) # Epochs 30 can be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b0eca-1678-4181-b177-f843b664c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perfomance from Total loss, Classificiation Loss and Regression Loss\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89ef45-cef8-4e12-85e5-6c2900ec4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "test_data = test.as_numpy_iterator()\n",
    "test_sample = test_data.next()\n",
    "\n",
    "yhat = facetracker.predict(test_sample[0])\n",
    "threshold = 0.5 # Can be updated\n",
    "predicted_labels = np.array(yhat[0] > threshold, dtype=np.uint8)\n",
    "\n",
    "wrong_predictions_indices = np.where(predicted_labels != test_sample[1][0].flatten())[0]\n",
    "wrong_images = test_sample[0][wrong_predictions_indices]\n",
    "true_labels = test_sample[1][0][wrong_predictions_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e6667-959d-4575-ab0e-10a9dc784f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL -Check if has wrong images\n",
    "# wrong_images.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74e9fb-6796-4776-903b-773c7d4eecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions above threshold\n",
    "fig, axes = plt.subplots(nrows=2, ncols=min(len(wrong_images), len(predicted_labels)) // 2, figsize=(15, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(wrong_images):  # Check if index is within the length of wrong_images\n",
    "        # Display the image\n",
    "        ax.imshow(wrong_images[i])\n",
    "\n",
    "        # Set the title with both true and predicted labels\n",
    "        ax.set_title(f'True Label: {true_labels[i]}\\nPredicted Label: {predicted_labels[i]}', color='red' if true_labels[i] != predicted_labels[i] else 'black')\n",
    "\n",
    "        # Hide the axes\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d179bee-0956-425a-91fa-259c7998595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all labels from prediction using test images\n",
    "fig, ax = plt.subplots(ncols=8, figsize=(20,20))\n",
    "for idx in range(8): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    sample_image = cv2.UMat(sample_image) # Convert to UMat\n",
    "    if yhat[0][idx] > 0.5:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    sample_image_display = np.asarray(sample_image.get()) # Convert to np.uint8 to be showed on imshow\n",
    "    ax[idx].imshow(sample_image_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905613f-a972-41e2-9a8a-14c37d191cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "facetracker.save('face_detection_v1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f161730-acf8-4e85-a329-e3a73beb8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "from tensorflow.keras.models import load_model\n",
    "facetracker = load_model('face_detection_v1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d249d-b3e0-40f9-a878-4bb48bcb1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Time Detection using trained model\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5:\n",
    "\n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('Face Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
